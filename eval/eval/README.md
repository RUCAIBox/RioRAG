# Evaluation

This folder contains scripts for evaluating RioRAG models.

## ðŸ“Š LongFact Evaluation

To evaluate with the **LongFact** checklist-style reward model, run:

```
python reward.py
```

This computes reward scores based on generated outputs.

## ðŸ“‹ RAGChecker Evaluation

For evaluations using **RAGChecker**, please refer to the official [RAGChecker repository](https://github.com/amazon-science/RAGChecker) for usage and setup instructions.
